import boto3
import os
import subprocess
from zipfile import ZipFile
from botocore.exceptions import ClientError
import argparse
import time

# Set up the AWS clients
s3 = boto3.client('s3')
cloudformation = boto3.client('cloudformation')

def parse_args():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Deploy Lambda function to S3.")
    parser.add_argument('--lambda-directory', required=True, help="Directory containing the Lambda function code")
    parser.add_argument('--zip-file-path', required=True, help="Path to the output ZIP file")
    parser.add_argument('--lambda-code-bucket', required=True, help="S3 bucket name to store Lambda code")
    parser.add_argument('--zip-file-key', required=True, help="Key (path) for the file in the S3 bucket")
    parser.add_argument('--region', required=True, help="AWS region where the bucket is located (e.g., us-east-1)")
    
    # Parameters for CloudFormation stack and file upload
    parser.add_argument('--stack-name', required=True, help="CloudFormation stack name")
    parser.add_argument('--template', required=True, help="Path to CloudFormation template")
    parser.add_argument('--source-bucket', required=True, help="Source S3 bucket name")
    parser.add_argument('--destination-bucket', required=True, help="Destination S3 bucket name")
    parser.add_argument('--test-file', required=True, help="Path to the test file to upload to S3")

    return parser.parse_args()

def zip_lambda_function(lambda_directory, zip_file_path):
    """Zips the contents of the Lambda function directory."""
    try:
        with ZipFile(zip_file_path, 'w') as zipf:
            for root, dirs, files in os.walk(lambda_directory):
                for file in files:
                    zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), lambda_directory))
        print(f"Lambda function code zipped successfully to {zip_file_path}.")
    except Exception as e:
        print(f"Error zipping the Lambda function code: {e}")
        raise

def create_s3_bucket_if_not_exists(bucket_name, region):
    """Creates an S3 bucket if it doesn't already exist."""
    try:
        s3.head_bucket(Bucket=bucket_name)  # Check if the bucket exists
        print(f"Bucket '{bucket_name}' already exists.")
    except ClientError as e:
        if e.response['Error']['Code'] == '404':
            print(f"Bucket '{bucket_name}' does not exist. Creating now...")
            # Create the bucket with the specified region
            s3.create_bucket(
                Bucket=bucket_name,
                CreateBucketConfiguration={'LocationConstraint': region}  # Specify the region
            )
            print(f"Bucket '{bucket_name}' created.")
        else:
            print(f"Error checking or creating bucket: {e}")
            raise e

def upload_zip_to_s3(bucket_name, zip_file_path, zip_file_key):
    """Uploads the ZIP file to the S3 bucket if it doesn't already exist."""
    try:
        # Check if the file already exists in S3
        s3.head_object(Bucket=bucket_name, Key=zip_file_key)
        print(f"File '{zip_file_key}' already exists in bucket '{bucket_name}'.")
    except s3.exceptions.ClientError as e:
        # If the file does not exist, upload the file
        print(f"Uploading file '{zip_file_key}' to bucket '{bucket_name}'...")
        s3.upload_file(zip_file_path, bucket_name, zip_file_key)
        print(f"File '{zip_file_key}' uploaded successfully.")

def check_stack_exists(stack_name):
    """Check if the CloudFormation stack exists."""
    try:
        response = cloudformation.describe_stacks(StackName=stack_name)
        return True  # Stack exists
    except cloudformation.exceptions.ClientError as e:
        if 'does not exist' in str(e):
            return False  # Stack does not exist
        else:
            raise e  # Re-raise any other error

def wait_for_stack_creation(stack_name): # boto3 inbuild functionality
    """Waits until the CloudFormation stack has been created or updated."""
    print(f"Waiting for CloudFormation stack '{stack_name}' to be created/updated...")
    while True:
        try:
            response = cloudformation.describe_stacks(StackName=stack_name)
            stack_status = response['Stacks'][0]['StackStatus']
            print(f"Current stack status: {stack_status}")
            if stack_status in ['CREATE_COMPLETE', 'UPDATE_COMPLETE']:
                print(f"Stack '{stack_name}' created/updated successfully.")
                break
            elif stack_status in ['CREATE_FAILED', 'UPDATE_FAILED']:
                print(f"Stack creation/update failed. Status: {stack_status}")
                raise Exception(f"Stack creation/update failed: {stack_status}")
            time.sleep(30)  # Wait before checking again
        except Exception as e:
            print(f"Error while checking stack status: {e}")
            raise

def run_aws_cli_command(command): # convert it to boto3
    """Run the AWS CLI command."""
    print(f"Running command: {' '.join(command)}")
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    
    if result.returncode != 0:
        print(f"Error running command: {' '.join(command)}")
        print(f"Error: {result.stderr.decode()}")
        raise Exception(f"Command failed: {' '.join(command)}")
    
    print(result.stdout.decode())  # Print successful output

def upload_file_to_s3(file_path, bucket_name, file_key, region):
    """Uploads a file to S3."""
    print(f"Uploading file '{file_path}' to S3 bucket '{bucket_name}' at key '{file_key}'...")
    s3.upload_file(file_path, bucket_name, file_key, ExtraArgs={'ACL': 'private'})
    print(f"File '{file_path}' uploaded successfully to '{bucket_name}/{file_key}'.")

def main():
    # Parse arguments from CLI
    args = parse_args()

    # Step 1: Create a ZIP file of the Lambda function code
    zip_lambda_function(args.lambda_directory, args.zip_file_path)

    # Step 2: Create the S3 bucket (for Lambda code) if it doesn't exist
    create_s3_bucket_if_not_exists(args.lambda_code_bucket, args.region)

    # Step 3: Upload the ZIP file to the S3 bucket if it doesn't already exist
    upload_zip_to_s3(args.lambda_code_bucket, args.zip_file_path, args.zip_file_key)

    # Step 4: Check if the CloudFormation stack exists and either create or update the stack
    stack_exists = check_stack_exists(args.stack_name)

    aws_cli_command = [
        'aws', 'cloudformation', 'create-stack' if not stack_exists else 'update-stack',
        '--stack-name', args.stack_name,
        '--template-body', f'file://{args.template}',
        '--capabilities', 'CAPABILITY_IAM',
        '--parameters',
        f'ParameterKey=SourceBucketName,ParameterValue={args.source_bucket}',
        f'ParameterKey=DestinationBucketName,ParameterValue={args.destination_bucket}',
        f'ParameterKey=LambdaCodeBucketName,ParameterValue={args.lambda_code_bucket}'
    ]

    run_aws_cli_command(aws_cli_command)

    # Step 5: Wait for CloudFormation stack creation or update to complete
    wait_for_stack_creation(args.stack_name)

    # Step 6: Upload the test file to the source bucket
    upload_file_to_s3(args.test_file, args.source_bucket, os.path.basename(args.test_file), args.region)

if __name__ == "__main__":
    main()





import subprocess
import argparse
import os

# Function to parse the command-line arguments
def parse_args():
    """Parses the command-line arguments."""
    # Define the argument parser for this script
    parser = argparse.ArgumentParser(description='Automated Deployment Script for Lambda + CloudFormation + S3')
    
    # Define all the arguments that the user will provide when running the script
    parser.add_argument('--lambda-directory', required=True, help='Path to the Lambda function directory')
    parser.add_argument('--zip-file-path', required=True, help='Path to store the ZIP file for Lambda function')
    parser.add_argument('--lambda-code-bucket', required=True, help='S3 bucket name for Lambda code')
    parser.add_argument('--zip-file-key', required=True, help='S3 key for the ZIP file')
    parser.add_argument('--region', required=True, help='AWS region')
    parser.add_argument('--stack-name', required=True, help='CloudFormation stack name')
    parser.add_argument('--template', required=True, help='Path to CloudFormation template')
    parser.add_argument('--source-bucket', required=True, help='Source S3 bucket for test file')
    parser.add_argument('--destination-bucket', required=True, help='Destination S3 bucket for Lambda triggers')
    parser.add_argument('--test-file', required=True, help='Path to the test file for uploading')

    # Return the parsed arguments
    return parser.parse_args()

# Function to run a shell command (e.g., AWS CLI command)
def run_subprocess(command):
    """Helper function to run a shell command and handle errors."""
    # Execute the command and capture stdout and stderr
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    
    # Check if the command was successful (returncode = 0 means success)
    if result.returncode != 0:
        # If there is an error, print the error message and raise an exception
        print(f"Error running command: {command}")
        print(f"Error: {result.stderr.decode()}")
        raise Exception(f"Command failed: {command}")
    
    # Print the output of the command (stdout)
    print(result.stdout.decode())

# Function to run the Python script (to zip and upload the Lambda function)
import subprocess
import shlex

import subprocess

import subprocess

def run_python_script(script, args):
    """Run the Python script to zip and upload Lambda function."""
    # Ensure you're calling python explicitly before the script
    command = ["python", script]  # Explicitly use 'python' to execute the script
    
    # Add each argument to the command list in the correct format
    for arg, value in vars(args).items():
        if arg not in ["stack_name", "template", "source_bucket", "destination_bucket", "test_file"]:
            command.append(f"--{arg}")
            command.append(str(value))  # Ensure the value is in string format

    # Print the command for debugging purposes
    print(f"Running command: {command}")  # Print the command directly
    
    # Run the command to execute the Python script
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if result.returncode != 0:
        print(f"Error running command: {command}")
        print(f"Error: {result.stderr.decode()}")
        raise Exception(f"Command failed: {command}")
    print(result.stdout.decode())  # Print the successful output from script.py





# Function to deploy or update the CloudFormation stack
def deploy_cloudformation(stack_name, template_path, region, source_bucket, dest_bucket, lambda_code_bucket):
    """Check if the stack exists and create or update it accordingly."""
    try:
        # Check if the stack already exists using the describe-stacks command
        print(f"[*] Checking if stack '{stack_name}' exists...")
        subprocess.run(
            ["aws", "cloudformation", "describe-stacks", "--stack-name", stack_name, "--region", region],
            check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        
        # If the stack exists, update it with new parameters
        print(f"[+] Stack '{stack_name}' exists. Updating stack...")
        subprocess.run([
            "aws", "cloudformation", "update-stack",
            "--stack-name", stack_name,
            "--template-body", f"file://{template_path}",
            "--parameters", 
            f"ParameterKey=SourceBucketName,ParameterValue={source_bucket}",
            f"ParameterKey=DestinationBucketName,ParameterValue={dest_bucket}",
            f"ParameterKey=LambdaCodeBucketName,ParameterValue={lambda_code_bucket}",
            "--capabilities", "CAPABILITY_IAM",
            "--region", region
        ], check=True)
    
    except subprocess.CalledProcessError:
        # If the stack does not exist, create a new stack
        print(f"[+] Stack '{stack_name}' does not exist. Creating stack...")
        subprocess.run([
            "aws", "cloudformation", "create-stack",
            "--stack-name", stack_name,
            "--template-body", f"file://{template_path}",
            "--parameters", 
            f"ParameterKey=SourceBucketName,ParameterValue={source_bucket}",
            f"ParameterKey=DestinationBucketName,ParameterValue={dest_bucket}",
            f"ParameterKey=LambdaCodeBucketName,ParameterValue={lambda_code_bucket}",
            "--capabilities", "CAPABILITY_IAM",
            "--region", region
        ], check=True)

# Function to upload the test file to the source S3 bucket
def upload_test_file(source_bucket, test_file, region):
    """Upload the test file to the source bucket."""
    print(f"[*] Uploading test file '{test_file}' to S3 bucket '{source_bucket}'...")
    
    # Use AWS CLI to upload the test file to the specified S3 bucket
    run_subprocess(["aws", "s3", "cp", test_file, f"s3://{source_bucket}/", "--region", region])
    
    # Confirm successful upload
    print("[✓] Test file uploaded successfully.")

# Main function to control the flow of the script
def main():
    # Step 1: Parse the command-line arguments
    args = parse_args()

    # Step 2: Run the Python script (script.py) to zip Lambda function and upload to S3
    print("[*] Running the Python script to zip Lambda function and upload to S3...")
    run_python_script('script.py', args)

    # Step 3: Deploy or update the CloudFormation stack
    print("[*] Deploying or updating CloudFormation stack...")
    deploy_cloudformation(
        args.stack_name,
        args.template,
        args.region,
        args.source_bucket,
        args.destination_bucket,
        args.lambda_code_bucket
    )

    # Step 4: Upload the test file to the source S3 bucket
    upload_test_file(args.source_bucket, args.test_file, args.region)

    # Print a success message indicating that the deployment is complete
    print("[✓] Deployment complete!")

# Entry point of the script when executed
if __name__ == "__main__":
    main()










# import boto3
# import os
# from zipfile import ZipFile
# from botocore.exceptions import NoCredentialsError, ClientError
# import argparse


# s3 = boto3.client('s3')

# def parse_args():
#     """Parse the command-line arguments."""
#     parser = argparse.ArgumentParser(description="Deploy Lambda function to S3.")
#     parser.add_argument('--lambda-directory', required=True, help="Directory containing the Lambda function code")
#     parser.add_argument('--zip-file-path', required=True, help="Path to the output ZIP file")
#     parser.add_argument('--lambda-code-bucket', required=True, help="S3 bucket name to store Lambda code")
#     parser.add_argument('--zip-file-key', required=True, help="Key (path) for the file in the S3 bucket")
#     parser.add_argument('--region', required=True, help="AWS region where the bucket is located (e.g., us-east-1)")
#     return parser.parse_args()

# def zip_lambda_function(lambda_directory, zip_file_path):
#     """Zips the contents of the Lambda function directory."""
#     try:
#         with ZipFile(zip_file_path, 'w') as zipf:
#             for root, dirs, files in os.walk(lambda_directory):
#                 for file in files:
#                     zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), lambda_directory))
#         print(f"Lambda function code zipped successfully to {zip_file_path}.")
#     except Exception as e:
#         print(f"Error zipping the Lambda function code: {e}")
#         raise

# def create_s3_bucket_if_not_exists(bucket_name, region):
#     """Creates an S3 bucket if it doesn't already exist."""
#     try:
#         s3.head_bucket(Bucket=bucket_name)  # Check if the bucket exists
#         print(f"Bucket '{bucket_name}' already exists.")
#     except ClientError as e:
#         if e.response['Error']['Code'] == '404':
#             print(f"Bucket '{bucket_name}' does not exist. Creating now...")
#             # Create the bucket with the specified region
#             s3.create_bucket(
#                 Bucket=bucket_name,
#                 CreateBucketConfiguration={'LocationConstraint': region}  # Specify the region
#             )
#             print(f"Bucket '{bucket_name}' created.")
#         else:
#             print(f"Error checking or creating bucket: {e}")
#             raise e

# def upload_zip_to_s3(bucket_name, zip_file_path, zip_file_key):
#     """Uploads the ZIP file to the S3 bucket if it doesn't already exist."""
#     try:
#         # Check if the file already exists in S3
#         s3.head_object(Bucket=bucket_name, Key=zip_file_key)
#         print(f"File '{zip_file_key}' already exists in bucket '{bucket_name}'.")
#     except s3.exceptions.ClientError as e:
#         # If the file does not exist, upload the file
#         print(f"Uploading file '{zip_file_key}' to bucket '{bucket_name}'...")
#         s3.upload_file(zip_file_path, bucket_name, zip_file_key)
#         print(f"File '{zip_file_key}' uploaded successfully.")

# def main():
#     # Parse arguments from CLI
#     args = parse_args()
    
#     # Step 1: Create a ZIP file of the Lambda function code
#     zip_lambda_function(args.lambda_directory, args.zip_file_path)

#     # Step 2: Create the S3 bucket (for Lambda code) if it doesn't exist
#     create_s3_bucket_if_not_exists(args.lambda_code_bucket, args.region)

#     # Step 3: Upload the ZIP file to the S3 bucket if it doesn't already exist
#     upload_zip_to_s3(args.lambda_code_bucket, args.zip_file_path, args.zip_file_key)

# if __name__ == "__main__":
#     main()








# import boto3
# import os
# from zipfile import ZipFile
# from botocore.exceptions import NoCredentialsError, ClientError
# import argparse

# # Set up the S3 client
# s3 = boto3.client('s3')

# def parse_args():
#     """Parse the command-line arguments."""
#     parser = argparse.ArgumentParser(description="Deploy Lambda function to S3.")
#     parser.add_argument('--lambda-directory', required=True, help="Directory containing the Lambda function code")
#     parser.add_argument('--zip-file-path', required=True, help="Path to the output ZIP file")
#     parser.add_argument('--lambda-code-bucket', required=True, help="S3 bucket name to store Lambda code")
#     parser.add_argument('--zip-file-key', required=True, help="Key (path) for the file in the S3 bucket")
#     parser.add_argument('--region', required=True, help="AWS region where the bucket is located (e.g., us-east-1)")
#     return parser.parse_args()

# def zip_lambda_function(lambda_directory, zip_file_path):
#     """Zips the contents of the Lambda function directory."""
#     with ZipFile(zip_file_path, 'w') as zipf:
#         for root, dirs, files in os.walk(lambda_directory):
#             for file in files:
#                 zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), lambda_directory))

# def create_s3_bucket_if_not_exists(bucket_name, region):
#     """Creates an S3 bucket if it doesn't already exist."""
#     try:
#         s3.head_bucket(Bucket=bucket_name)  # Check if the bucket exists
#         print(f"Bucket '{bucket_name}' already exists.")
#     except ClientError as e:
#         if e.response['Error']['Code'] == '404':
#             print(f"Bucket '{bucket_name}' does not exist. Creating now...")
#             # Create the bucket with the specified region
#             s3.create_bucket(
#                 Bucket=bucket_name,
#                 CreateBucketConfiguration={'LocationConstraint': region}  # Specify the region
#             )
#             print(f"Bucket '{bucket_name}' created.")
#         else:
#             raise e  # Re-raise other errors

# def upload_zip_to_s3(bucket_name, zip_file_path, zip_file_key):
#     """Uploads the ZIP file to the S3 bucket if it doesn't already exist."""
#     try:
#         # Check if the file already exists in S3
#         s3.head_object(Bucket=bucket_name, Key=zip_file_key)
#         print(f"File '{zip_file_key}' already exists in bucket '{bucket_name}'.")
#     except s3.exceptions.ClientError as e:
#         # If the file does not exist, upload the file
#         print(f"Uploading file '{zip_file_key}' to bucket '{bucket_name}'...")
#         s3.upload_file(zip_file_path, bucket_name, zip_file_key)
#         print(f"File '{zip_file_key}' uploaded successfully.")

# def main():
#     # Parse arguments from CLI
#     args = parse_args()
    
#     # Step 1: Create a ZIP file of the Lambda function code
#     zip_lambda_function(args.lambda_directory, args.zip_file_path)

#     # Step 2: Create the S3 bucket (for Lambda code) if it doesn't exist
#     create_s3_bucket_if_not_exists(args.lambda_code_bucket, args.region)

#     # Step 3: Upload the ZIP file to the S3 bucket if it doesn't already exist
#     upload_zip_to_s3(args.lambda_code_bucket, args.zip_file_path, args.zip_file_key)

# if __name__ == "__main__":
#     main()